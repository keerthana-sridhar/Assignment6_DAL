{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "opuwSMGX2B6w"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = '/content/archive (1).zip'  # Change this to your actual ZIP file location\n",
        "extract_folder = '/content/dataset'  # Destination folder\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/dataset/UCI_Credit_Card.csv'\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Show basic info and first few rows\n",
        "print(df.info())\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLJLU5mI4nmv",
        "outputId": "699a8fa9-8c76-42d3-8cb4-393e8e650f99"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 30000 entries, 0 to 29999\n",
            "Data columns (total 25 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   ID                          30000 non-null  int64  \n",
            " 1   LIMIT_BAL                   30000 non-null  float64\n",
            " 2   SEX                         30000 non-null  int64  \n",
            " 3   EDUCATION                   30000 non-null  int64  \n",
            " 4   MARRIAGE                    30000 non-null  int64  \n",
            " 5   AGE                         30000 non-null  int64  \n",
            " 6   PAY_0                       30000 non-null  int64  \n",
            " 7   PAY_2                       30000 non-null  int64  \n",
            " 8   PAY_3                       30000 non-null  int64  \n",
            " 9   PAY_4                       30000 non-null  int64  \n",
            " 10  PAY_5                       30000 non-null  int64  \n",
            " 11  PAY_6                       30000 non-null  int64  \n",
            " 12  BILL_AMT1                   30000 non-null  float64\n",
            " 13  BILL_AMT2                   30000 non-null  float64\n",
            " 14  BILL_AMT3                   30000 non-null  float64\n",
            " 15  BILL_AMT4                   30000 non-null  float64\n",
            " 16  BILL_AMT5                   30000 non-null  float64\n",
            " 17  BILL_AMT6                   30000 non-null  float64\n",
            " 18  PAY_AMT1                    30000 non-null  float64\n",
            " 19  PAY_AMT2                    30000 non-null  float64\n",
            " 20  PAY_AMT3                    30000 non-null  float64\n",
            " 21  PAY_AMT4                    30000 non-null  float64\n",
            " 22  PAY_AMT5                    30000 non-null  float64\n",
            " 23  PAY_AMT6                    30000 non-null  float64\n",
            " 24  default.payment.next.month  30000 non-null  int64  \n",
            "dtypes: float64(13), int64(12)\n",
            "memory usage: 5.7 MB\n",
            "None\n",
            "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
            "0   1    20000.0    2          2         1   24      2      2     -1     -1   \n",
            "1   2   120000.0    2          2         2   26     -1      2      0      0   \n",
            "2   3    90000.0    2          2         2   34      0      0      0      0   \n",
            "3   4    50000.0    2          2         1   37      0      0      0      0   \n",
            "4   5    50000.0    1          2         1   57     -1      0     -1      0   \n",
            "\n",
            "   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
            "0  ...        0.0        0.0        0.0       0.0     689.0       0.0   \n",
            "1  ...     3272.0     3455.0     3261.0       0.0    1000.0    1000.0   \n",
            "2  ...    14331.0    14948.0    15549.0    1518.0    1500.0    1000.0   \n",
            "3  ...    28314.0    28959.0    29547.0    2000.0    2019.0    1200.0   \n",
            "4  ...    20940.0    19146.0    19131.0    2000.0   36681.0   10000.0   \n",
            "\n",
            "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default.payment.next.month  \n",
            "0       0.0       0.0       0.0                           1  \n",
            "1    1000.0       0.0    2000.0                           1  \n",
            "2    1000.0    1000.0    5000.0                           0  \n",
            "3    1100.0    1069.0    1000.0                           0  \n",
            "4    9000.0     689.0     679.0                           0  \n",
            "\n",
            "[5 rows x 25 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First I inspected the dataset and selected 3 columns to introduce 5-10 percent MAR."
      ],
      "metadata": {
        "id": "3NArhq86W0Mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your loaded dataset\n",
        "new_dataset = df.copy()\n",
        "columns_to_nan = ['AGE', 'BILL_AMT1', 'BILL_AMT2']  # Select 2-3 numerical columns\n",
        "\n",
        "for col in columns_to_nan:\n",
        "    missing_percent = np.random.uniform(0.05, 0.10)  # Random percentage between 5 and 10%\n",
        "    n_missing = int(missing_percent * len(new_dataset))\n",
        "    missing_indices = np.random.choice(new_dataset.index, n_missing, replace=False)\n",
        "    new_dataset.loc[missing_indices, col] = np.nan\n",
        "\n",
        "print(new_dataset.isna().mean())  # Check fraction of missing values per column\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpbG1pbg3Wyu",
        "outputId": "274210c2-8385-4667-ee86-883ad4982e73"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID                            0.000000\n",
            "LIMIT_BAL                     0.000000\n",
            "SEX                           0.000000\n",
            "EDUCATION                     0.000000\n",
            "MARRIAGE                      0.000000\n",
            "AGE                           0.051967\n",
            "PAY_0                         0.000000\n",
            "PAY_2                         0.000000\n",
            "PAY_3                         0.000000\n",
            "PAY_4                         0.000000\n",
            "PAY_5                         0.000000\n",
            "PAY_6                         0.000000\n",
            "BILL_AMT1                     0.093533\n",
            "BILL_AMT2                     0.073433\n",
            "BILL_AMT3                     0.000000\n",
            "BILL_AMT4                     0.000000\n",
            "BILL_AMT5                     0.000000\n",
            "BILL_AMT6                     0.000000\n",
            "PAY_AMT1                      0.000000\n",
            "PAY_AMT2                      0.000000\n",
            "PAY_AMT3                      0.000000\n",
            "PAY_AMT4                      0.000000\n",
            "PAY_AMT5                      0.000000\n",
            "PAY_AMT6                      0.000000\n",
            "default.payment.next.month    0.000000\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# Part A: Data Preprocessing and Imputation\n",
        "# =========================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ---------------------------\n",
        "# 0. Load dataset\n",
        "# ---------------------------\n",
        "np.random.seed(42)\n",
        "dataset = new_dataset.copy()\n",
        "\n",
        "\n",
        "for col in ['AGE', 'BILL_AMT1','BILL_AMT2']:\n",
        "    mask = np.random.rand(len(dataset)) < 0.05\n",
        "    dataset.loc[mask, col] = np.nan\n",
        "\n",
        "print(\"Missing values after introducing MAR:\")\n",
        "print(dataset.isna().sum().sort_values(ascending=False))\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTERoJgYHdki",
        "outputId": "3688d6e9-c4b6-40a3-8ab6-6fe189aa51b7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values after introducing MAR:\n",
            "BILL_AMT1                     4199\n",
            "BILL_AMT2                     3555\n",
            "AGE                           2954\n",
            "ID                               0\n",
            "LIMIT_BAL                        0\n",
            "MARRIAGE                         0\n",
            "PAY_0                            0\n",
            "EDUCATION                        0\n",
            "SEX                              0\n",
            "PAY_3                            0\n",
            "PAY_2                            0\n",
            "PAY_5                            0\n",
            "PAY_4                            0\n",
            "PAY_6                            0\n",
            "BILL_AMT3                        0\n",
            "BILL_AMT4                        0\n",
            "BILL_AMT5                        0\n",
            "BILL_AMT6                        0\n",
            "PAY_AMT1                         0\n",
            "PAY_AMT2                         0\n",
            "PAY_AMT3                         0\n",
            "PAY_AMT4                         0\n",
            "PAY_AMT5                         0\n",
            "PAY_AMT6                         0\n",
            "default.payment.next.month       0\n",
            "dtype: int64\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imputing with median"
      ],
      "metadata": {
        "id": "khcQjGp-3oUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Dataset A: Simple Median Imputation\n",
        "# ---------------------------\n",
        "dataset_a = dataset.copy()\n",
        "\n",
        "for col in dataset_a.columns:\n",
        "    if dataset_a[col].isna().sum() > 0:\n",
        "        median_value = dataset_a[col].median()\n",
        "        dataset_a[col].fillna(median_value, inplace=True)\n",
        "\n",
        "print(\"Dataset A: Missing values after median imputation:\")\n",
        "print(dataset_a.isna().sum())\n",
        "print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks-zdtIXHglv",
        "outputId": "a5a2e9b0-8956-4121-e9c0-3695c2f04f88"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset A: Missing values after median imputation:\n",
            "ID                            0\n",
            "LIMIT_BAL                     0\n",
            "SEX                           0\n",
            "EDUCATION                     0\n",
            "MARRIAGE                      0\n",
            "AGE                           0\n",
            "PAY_0                         0\n",
            "PAY_2                         0\n",
            "PAY_3                         0\n",
            "PAY_4                         0\n",
            "PAY_5                         0\n",
            "PAY_6                         0\n",
            "BILL_AMT1                     0\n",
            "BILL_AMT2                     0\n",
            "BILL_AMT3                     0\n",
            "BILL_AMT4                     0\n",
            "BILL_AMT5                     0\n",
            "BILL_AMT6                     0\n",
            "PAY_AMT1                      0\n",
            "PAY_AMT2                      0\n",
            "PAY_AMT3                      0\n",
            "PAY_AMT4                      0\n",
            "PAY_AMT5                      0\n",
            "PAY_AMT6                      0\n",
            "default.payment.next.month    0\n",
            "dtype: int64\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1137143021.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  dataset_a[col].fillna(median_value, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Use the Median for Imputation?\n",
        "\n",
        "- The **median** is the middle value of a sorted column and is **less sensitive to outliers** than the mean.\n",
        "- Using the **mean** can be skewed by extreme values (very high or very low), whereas the **median** provides a more robust measure of central tendency.\n",
        "\n"
      ],
      "metadata": {
        "id": "ncfAYP7cOKNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Linear Regression\n"
      ],
      "metadata": {
        "id": "_XhmtHPvOMiD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset B: Linear Regression Imputation\n",
        "\n",
        "I created a copy of the dataset and focused on the column `BILL_AMT1`, which had missing values.  \n",
        "I separated the data into rows where `BILL_AMT1` was known (to train a model) and rows where it was missing (to predict). The reason I chose BILL_AMNT1 was that AGE may have predictions that are not biologically possible and hence I chose BILL_AMNT1.\n",
        "\n",
        "I filled the NAN columns with their respective medians so that we can now compare the dataset with the change in the BILL_AMT1 column alone and also use the columns AGE and BILL_AMT2 to predict BILL_AMT1. I excluded the ID, BILL_AMNT1 and the default payment columns from the regressor.\n",
        "\n",
        "\n",
        "After cleaning the training data, I trained a **Linear Regression** model to learn the relationship between the predictors and `BILL_AMT1`.  \n",
        "\n",
        "Finally, I used this model to predict the missing values and filled them in the dataset.  \n",
        "Now, `BILL_AMT1` has no missing values, imputed based on other related features.\n"
      ],
      "metadata": {
        "id": "n0r0Xop5RZAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 2. Dataset B: Linear Regression Imputation\n",
        "# ---------------------------\n",
        "dataset_b = dataset.copy()\n",
        "target_col = 'BILL_AMT1'\n",
        "\n",
        "# ---------------------------\n",
        "# 0. Fill other columns with median first\n",
        "# ---------------------------\n",
        "for col in ['AGE', 'BILL_AMT2']:\n",
        "    median_val = dataset_b[col].median()\n",
        "    dataset_b[col].fillna(median_val, inplace=True)\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Split into known and unknown target\n",
        "# ---------------------------\n",
        "train_data = dataset_b[dataset_b[target_col].notna()]\n",
        "test_data = dataset_b[dataset_b[target_col].isna()]\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Select features for regression\n",
        "# Exclude target and target variable column\n",
        "exclude_cols = ['ID',target_col, 'default.payment.next.month']  # AGE and BILL_AMT2 already filled\n",
        "features = [col for col in dataset_b.columns if col not in exclude_cols]\n",
        "\n",
        "X_train = train_data[features]\n",
        "y_train = train_data[target_col]\n",
        "X_test = test_data[features]\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Fit Linear Regression and predict\n",
        "# ---------------------------\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "predicted_values = lr.predict(X_test)\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Fill predicted values into the dataset\n",
        "# ---------------------------\n",
        "dataset_b.loc[X_test.index, target_col] = predicted_values\n",
        "\n",
        "# ---------------------------\n",
        "# 5. Display remaining missing values\n",
        "# ---------------------------\n",
        "print(\"Dataset B: Missing values after Linear Regression and median imputation for other columns:\")\n",
        "print(dataset_b.isna().sum())\n",
        "print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGeb2JCRSz5F",
        "outputId": "a6f54e86-6c41-4ce2-e120-90cb1a892c88"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset B: Missing values after Linear Regression and median imputation for other columns:\n",
            "ID                            0\n",
            "LIMIT_BAL                     0\n",
            "SEX                           0\n",
            "EDUCATION                     0\n",
            "MARRIAGE                      0\n",
            "AGE                           0\n",
            "PAY_0                         0\n",
            "PAY_2                         0\n",
            "PAY_3                         0\n",
            "PAY_4                         0\n",
            "PAY_5                         0\n",
            "PAY_6                         0\n",
            "BILL_AMT1                     0\n",
            "BILL_AMT2                     0\n",
            "BILL_AMT3                     0\n",
            "BILL_AMT4                     0\n",
            "BILL_AMT5                     0\n",
            "BILL_AMT6                     0\n",
            "PAY_AMT1                      0\n",
            "PAY_AMT2                      0\n",
            "PAY_AMT3                      0\n",
            "PAY_AMT4                      0\n",
            "PAY_AMT5                      0\n",
            "PAY_AMT6                      0\n",
            "default.payment.next.month    0\n",
            "dtype: int64\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2315515431.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  dataset_b[col].fillna(median_val, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qrKrG3UdTRp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN Regression Imputation\n",
        "\n",
        "Here's how I handled missing values in `BILL_AMT1` using KNN regression:\n",
        "\n",
        "1. **Copy the dataset**  \n",
        "   I created a fresh copy of the dataset to work on, so the original remains unchanged.\n",
        "\n",
        "2. **Fill missing values in other columns**  \n",
        "   I filled missing values in `AGE` and `BILL_AMT2` with their median values.  \n",
        "   This ensures that only `BILL_AMT1` (the target) still has missing entries.\n",
        "\n",
        "3. **Split data into training and prediction sets**  \n",
        "   I separated the rows where `BILL_AMT1` is known (to train the model) and where it is missing (to predict).\n",
        "\n",
        "4. **Select features for KNN regression**  \n",
        "   I excluded the target column (`BILL_AMT1`) and the label column (`default.payment.next.month`).  \n",
        "   All other columns are used as features.\n",
        "\n",
        "5. **Tune the number of neighbors (k)**  \n",
        "   I split the training data into a smaller training set and a validation set.  \n",
        "   I tried different values of `k` and selected the one that gave the lowest mean squared error on the validation set.\n",
        "\n",
        "6. **Train the final KNN regressor and predict missing values**  \n",
        "   I trained KNN on the full training set using the best `k`.  \n",
        "   I predicted the missing `BILL_AMT1` values and clipped any negative predictions to zero.\n",
        "\n",
        "7. **Fill in predicted values**  \n",
        "   I replaced the missing `BILL_AMT1` entries with the predicted values.\n",
        "\n",
        "8. **Check for remaining missing values**  \n",
        "   Finally, I verified that there are no remaining missing values in the dataset and printed the result.\n"
      ],
      "metadata": {
        "id": "6re4R-LrVL_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 3. Dataset C: Non-linear (KNN) Regression Imputation with Scaling\n",
        "# ---------------------------\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "dataset_c = dataset.copy()\n",
        "target_col = 'BILL_AMT1'\n",
        "\n",
        "# ---------------------------\n",
        "# 0. Fill other columns with median first\n",
        "# ---------------------------\n",
        "for col in ['AGE', 'BILL_AMT2']:\n",
        "    median_val = dataset_c[col].median()\n",
        "    dataset_c[col].fillna(median_val, inplace=True)\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Split into known and unknown target\n",
        "# ---------------------------\n",
        "train_data = dataset_c[dataset_c[target_col].notna()]\n",
        "test_data = dataset_c[dataset_c[target_col].isna()]\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Features for KNN regression\n",
        "# ---------------------------\n",
        "exclude_cols = ['ID', target_col, 'default.payment.next.month']\n",
        "features = [col for col in dataset_c.columns if col not in exclude_cols]\n",
        "\n",
        "X_train_full = train_data[features]\n",
        "y_train_full = train_data[target_col]\n",
        "X_test_c = test_data[features]\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Scale features\n",
        "# ---------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled_full = scaler.fit_transform(X_train_full)\n",
        "X_test_scaled = scaler.transform(X_test_c)\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Tune n_neighbors using a small validation split\n",
        "# ---------------------------\n",
        "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
        "    X_train_scaled_full, y_train_full, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "neighbor_options = [3, 5, 7, 9, 11]\n",
        "best_k = 3\n",
        "best_mse = float('inf')\n",
        "\n",
        "for k in neighbor_options:\n",
        "    knn = KNeighborsRegressor(n_neighbors=k, weights='distance')\n",
        "    knn.fit(X_train_split, y_train_split)\n",
        "    y_pred_val = knn.predict(X_val)\n",
        "    mse = mean_squared_error(y_val, y_pred_val)\n",
        "    if mse < best_mse:\n",
        "        best_mse = mse\n",
        "        best_k = k\n",
        "\n",
        "# ---------------------------\n",
        "# 5. Train final KNN on full scaled training set and predict\n",
        "# ---------------------------\n",
        "final_knn = KNeighborsRegressor(n_neighbors=best_k, weights='distance')\n",
        "final_knn.fit(X_train_scaled_full, y_train_full)\n",
        "\n",
        "predicted_values = final_knn.predict(X_test_scaled)\n",
        "predicted_values = np.maximum(predicted_values, 0)  # clip negative values\n",
        "\n",
        "dataset_c.loc[X_test_c.index, target_col] = predicted_values\n",
        "\n",
        "# ---------------------------\n",
        "# 6. Display remaining missing values\n",
        "# ---------------------------\n",
        "print(\"Dataset C: Missing values after KNN Regression and median imputation for other columns:\")\n",
        "print(dataset_c.isna().sum())\n",
        "print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqkBUvSITbfL",
        "outputId": "debd17ab-2e4b-475b-bd20-ed70e3037092"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-243682761.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  dataset_c[col].fillna(median_val, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset C: Missing values after KNN Regression and median imputation for other columns:\n",
            "ID                            0\n",
            "LIMIT_BAL                     0\n",
            "SEX                           0\n",
            "EDUCATION                     0\n",
            "MARRIAGE                      0\n",
            "AGE                           0\n",
            "PAY_0                         0\n",
            "PAY_2                         0\n",
            "PAY_3                         0\n",
            "PAY_4                         0\n",
            "PAY_5                         0\n",
            "PAY_6                         0\n",
            "BILL_AMT1                     0\n",
            "BILL_AMT2                     0\n",
            "BILL_AMT3                     0\n",
            "BILL_AMT4                     0\n",
            "BILL_AMT5                     0\n",
            "BILL_AMT6                     0\n",
            "PAY_AMT1                      0\n",
            "PAY_AMT2                      0\n",
            "PAY_AMT3                      0\n",
            "PAY_AMT4                      0\n",
            "PAY_AMT5                      0\n",
            "PAY_AMT6                      0\n",
            "default.payment.next.month    0\n",
            "dtype: int64\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned I created a dataset D that deleted all rows that had NANs."
      ],
      "metadata": {
        "id": "8sNB72UmVPBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 4. Dataset D: Listwise Deletion (after median imputation)\n",
        "# ---------------------------\n",
        "dataset_d = dataset.copy()\n",
        "\n",
        "# 0. Median imputation for AGE and BILL_AMT2\n",
        "for col in ['AGE', 'BILL_AMT2']:\n",
        "    median_val = dataset_d[col].median()\n",
        "    dataset_d[col].fillna(median_val, inplace=True)\n",
        "\n",
        "# 1. Drop any remaining rows with missing values\n",
        "dataset_d = dataset_d.dropna()\n",
        "\n",
        "print(f\"Dataset D shape after median imputation (AGE, BILL_AMT2) and listwise deletion: {dataset_d.shape}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# =========================================\n",
        "# Part B: Logistic Regression Classification\n",
        "# =========================================\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "target_col_class = \"default.payment.next.month\"\n",
        "\n",
        "def split_features_target(dataset, target_col):\n",
        "    X = dataset.drop(columns=[target_col])\n",
        "    y = dataset[target_col]\n",
        "    return X, y\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dse34AIfH-Ek",
        "outputId": "65ebe499-db33-4256-b8ae-93f7ba7cbf87"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset D shape after median imputation (AGE, BILL_AMT2) and listwise deletion: (25801, 25)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3647020101.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  dataset_d[col].fillna(median_val, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then I created the train and test sets with the target variable as the default next month payment for classifying whether the user defaults or not. Used a standard 0.2 test train split. Also I standardised the datasets using their mean and variances. This is to ensure the model is not affected by different feature scales."
      ],
      "metadata": {
        "id": "_Zv6-IudVdZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- Split datasets ---\n",
        "X_a, y_a = split_features_target(dataset_a, target_col_class)\n",
        "X_b, y_b = split_features_target(dataset_b, target_col_class)\n",
        "X_c, y_c = split_features_target(dataset_c, target_col_class)\n",
        "X_d, y_d = split_features_target(dataset_d, target_col_class)\n",
        "\n",
        "# --- Train/test split ---\n",
        "X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(X_a, y_a, test_size=0.2, random_state=42)\n",
        "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X_b, y_b, test_size=0.2, random_state=42)\n",
        "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_c, y_c, test_size=0.2, random_state=42)\n",
        "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_d, y_d, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Standardize ---\n",
        "scaler = StandardScaler()\n",
        "X_train_a = scaler.fit_transform(X_train_a)\n",
        "X_test_a = scaler.transform(X_test_a)\n",
        "\n",
        "X_train_b = scaler.fit_transform(X_train_b)\n",
        "X_test_b = scaler.transform(X_test_b)\n",
        "\n",
        "X_train_c = scaler.fit_transform(X_train_c)\n",
        "X_test_c = scaler.transform(X_test_c)\n",
        "\n",
        "X_train_d = scaler.fit_transform(X_train_d)\n",
        "X_test_d = scaler.transform(X_test_d)\n",
        "\n",
        "# --- Train Logistic Regression and evaluate ---\n",
        "def train_evaluate_logreg(X_train, X_test, y_train, y_test, dataset_name):\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "    print(f\"=== Dataset {dataset_name} ===\")\n",
        "    print(f\"Accuracy: {acc:.3f}\")\n",
        "    print(\"Class-wise metrics:\")\n",
        "    for label, metrics in report.items():\n",
        "        if isinstance(metrics, dict):  # Skip 'accuracy', handle dict classes\n",
        "            precision = metrics['precision']\n",
        "            recall = metrics['recall']\n",
        "            f1 = metrics['f1-score']\n",
        "            support = metrics['support']\n",
        "            print(f\"  {label}: Precision={precision:.3f}, Recall={recall:.3f}, F1-score={f1:.3f}, Support={support}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "train_evaluate_logreg(X_train_a, X_test_a, y_train_a, y_test_a, \"A\")\n",
        "train_evaluate_logreg(X_train_b, X_test_b, y_train_b, y_test_b, \"B\")\n",
        "train_evaluate_logreg(X_train_c, X_test_c, y_train_c, y_test_c, \"C\")\n",
        "train_evaluate_logreg(X_train_d, X_test_d, y_train_d, y_test_d, \"D\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5y8YNkxIgwK",
        "outputId": "408a9ae0-4c0f-4342-e0b7-b2cdb5e5435b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Dataset A ===\n",
            "Accuracy: 0.810\n",
            "Class-wise metrics:\n",
            "  0: Precision=0.819, Recall=0.972, F1-score=0.889, Support=4687.0\n",
            "  1: Precision=0.697, Recall=0.233, F1-score=0.349, Support=1313.0\n",
            "  macro avg: Precision=0.758, Recall=0.602, F1-score=0.619, Support=6000.0\n",
            "  weighted avg: Precision=0.792, Recall=0.810, F1-score=0.771, Support=6000.0\n",
            "\n",
            "\n",
            "=== Dataset B ===\n",
            "Accuracy: 0.809\n",
            "Class-wise metrics:\n",
            "  0: Precision=0.819, Recall=0.971, F1-score=0.888, Support=4687.0\n",
            "  1: Precision=0.692, Recall=0.233, F1-score=0.349, Support=1313.0\n",
            "  macro avg: Precision=0.756, Recall=0.602, F1-score=0.619, Support=6000.0\n",
            "  weighted avg: Precision=0.791, Recall=0.809, F1-score=0.770, Support=6000.0\n",
            "\n",
            "\n",
            "=== Dataset C ===\n",
            "Accuracy: 0.809\n",
            "Class-wise metrics:\n",
            "  0: Precision=0.819, Recall=0.970, F1-score=0.888, Support=4687.0\n",
            "  1: Precision=0.690, Recall=0.235, F1-score=0.351, Support=1313.0\n",
            "  macro avg: Precision=0.754, Recall=0.603, F1-score=0.620, Support=6000.0\n",
            "  weighted avg: Precision=0.791, Recall=0.809, F1-score=0.771, Support=6000.0\n",
            "\n",
            "\n",
            "=== Dataset D ===\n",
            "Accuracy: 0.805\n",
            "Class-wise metrics:\n",
            "  0: Precision=0.817, Recall=0.967, F1-score=0.886, Support=4029.0\n",
            "  1: Precision=0.661, Recall=0.226, F1-score=0.337, Support=1132.0\n",
            "  macro avg: Precision=0.739, Recall=0.597, F1-score=0.611, Support=5161.0\n",
            "  weighted avg: Precision=0.783, Recall=0.805, F1-score=0.765, Support=5161.0\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Logistic Regression Results Across Imputation Methods**\n",
        "\n",
        "| **Model** | **Imputation Method**            | **Accuracy** | **Precision (1)** | **F1-score (1)** | **Comment** |\n",
        "|------------|----------------------------------|---------------|-------------------|------------------|--------------|\n",
        "| **A** | Median Imputation | 0.810 | 0.697 | 0.349 | Baseline model — performs well for MCAR data, but minority-class recall remains low. |\n",
        "| **B** | Linear Regression Imputation | 0.809 | 0.692 | 0.349 | Captures linear relationships; performance similar to baseline. |\n",
        "| **C** | KNN (Non-linear) Imputation | 0.809 | 0.690 | 0.351 | Non-linear model; performance similar to linear regression, underlying relations mostly linear. |\n",
        "| **D** | Listwise Deletion | 0.805 | 0.661 | 0.337 | Loses data due to deletion; minority-class recall drops, potential bias and reduced representativeness. |\n"
      ],
      "metadata": {
        "id": "A-kudxdW87wM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Efficacy Discussion\n",
        "\n",
        "1. **Listwise Deletion vs. Imputation**  \n",
        "   - Model D (Listwise Deletion) removes rows with missing values, which reduces the dataset size and may lead to information loss.  \n",
        "   - Models A–C retain all rows by imputing missing values, keeping more data for training.  \n",
        "   - Even if imputation introduces some noise, the models often perform better overall because more data is available."
      ],
      "metadata": {
        "id": "NHAsXx7LWkJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Linear vs. Non-Linear Regression Imputation**  \n",
        "   - Model B uses linear regression to impute missing values, assuming a linear relationship between the target feature and predictors.  \n",
        "   - Model C uses KNN (non-linear regression), which can capture more complex, non-linear patterns.  \n",
        "   - If the imputed feature has non-linear relationships with predictors, KNN generally performs better.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "--3utlJdWo9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Imputation Models (A–C)**\n",
        "\n",
        "- Maintain the dataset’s overall size and diversity.  \n",
        "- **Model A (Median Imputation):** Performs best when data are *Missing Completely at Random (MCAR)*.  \n",
        "- **Model B (Linear Regression Imputation):** Captures inter-variable relationships effectively under *Missing at Random (MAR)* conditions, yielding modest improvements in precision and F1-score.  \n",
        "- **Model C (Non-linear KNN Imputation):** Adds flexibility but shows comparable outcomes — suggesting that the link between `BILL_AMT1` and other predictors is mostly linear.  \n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion and Recommendation\n",
        "\n",
        "- **Recommended Approach:** *Linear regression imputation (Model B)* — consistently offers slight improvements in minority-class performance with minimal added complexity. If we notice a major increase in usin Method 3, then for that dataset, we could assume a non linear relationship and use *(Model C)* .\n",
        "- **Median Imputation:** Suitable for quick baselines or when missing data are minimal.  \n",
        "- **Listwise Deletion:** Use only when missingness is below 2% or when simplicity and interpretability take priority over potential bias.  \n",
        "\n",
        "> **Summary:** Regression-based imputation effectively preserves both information and model stability.  \n",
        "> Accuracy alone should not be the sole measure of performance in imbalanced classification problems.\n"
      ],
      "metadata": {
        "id": "Or3zAuNLWv0P"
      }
    }
  ]
}